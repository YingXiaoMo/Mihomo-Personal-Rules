import argparse
import os
import re
import sys
import glob
import time
from pathlib import Path

import lightgbm as lgb
import numpy as np
import pandas as pd
import requests
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler


SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent

DEFAULT_DATA_DIR = PROJECT_ROOT
DEFAULT_MODEL_PATH = PROJECT_ROOT / "Model.bin"
CACHE_DIR = SCRIPT_DIR / "cache"
GO_SOURCE_CACHE_PATH = CACHE_DIR / "transform.go.cache"
GO_SOURCE_URL = "https://raw.githubusercontent.com/vernesong/mihomo/Alpha/component/smart/lightgbm/transform.go"

IGNORED_FEATURES = [
    'upload_mb', 
    'history_upload_mb',
    'maxuploadrate_kb',         
    'history_maxuploadrate_kb',
    
    'asn_feature', 
    'country_feature', 
    'address_feature', 
    'port_feature', 
    'connection_type_feature',
    
    'traffic_density', 
    'traffic_ratio'
]

CONTINUOUS_FEATURES = [
    'connect_time', 'latency', 
    'download_mb', 'history_download_mb', 
    'maxdownloadrate_kb', 'history_maxdownloadrate_kb', 
    'duration_minutes', 'last_used_seconds',
    'asn_hash', 'host_hash', 'ip_hash', 'geoip_hash',
    'upload_mb', 'history_upload_mb', 'maxuploadrate_kb', 'history_maxuploadrate_kb',
    'traffic_density', 'traffic_ratio'
]

COUNT_FEATURES = ['success', 'failure']


LGBM_PARAMS = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'n_estimators': 10000,       
    'learning_rate': 0.03,
    'num_leaves': 63,
    'max_depth': -1,
    'min_child_samples': 10,    
    'subsample': 0.85,
    'colsample_bytree': 0.85,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': -1
}


def print_separator(title=None):
    if title:
        print("=" * 60)
        print(f"{title}")
        print("=" * 60)
    else:
        print("=" * 60)

class GoTransformParser:
    """
    Go æºç è§£æå™¨ (å¢å¼ºç‰ˆ)
    """
    def __init__(self, content: str):
        self.content = content
        self.feature_order = self._parse_feature_order()

    def _parse_feature_order(self):
        print("å¼€å§‹è§£æ getDefaultFeatureOrder å‡½æ•°...")
        pattern = (
            r'func getDefaultFeatureOrder\(\) map\[int\]string \{\s*'
            r'return map\[int\]string\{(.*?)\}\s*\}'
        )
        match = re.search(pattern, self.content, re.DOTALL)
        
        if not match:
            print("è­¦å‘Š: æœªèƒ½åœ¨æºç ä¸­æ‰¾åˆ°ç‰¹å¾å®šä¹‰å—ï¼Œä½¿ç”¨å†…ç½®åå¤‡é…ç½®")
            return self._get_fallback_config()
        
        body = match.group(1)
        pairs = re.findall(r'(\d+):\s*"([^"]+)"', body)
        
        if not pairs:
            print("è­¦å‘Š: è§£æåˆ°çš„ç‰¹å¾åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨åå¤‡é…ç½®")
            return self._get_fallback_config()
            
        feature_map = {int(idx): name for idx, name in pairs}
        print(f"æˆåŠŸè§£æ {len(feature_map)} ä¸ªç‰¹å¾")
        return feature_map

    def _get_fallback_config(self):
        features = [
            'success', 'failure', 'connect_time', 'latency', 'upload_mb', 
            'history_upload_mb', 'maxuploadrate_kb', 'history_maxuploadrate_kb',
            'download_mb', 'history_download_mb', 'maxdownloadrate_kb', 
            'history_maxdownloadrate_kb', 'duration_minutes', 'last_used_seconds', 
            'is_udp', 'is_tcp', 'asn_feature', 'country_feature', 'address_feature', 
            'port_feature', 'traffic_ratio', 'traffic_density', 
            'connection_type_feature', 'asn_hash', 'host_hash', 'ip_hash', 'geoip_hash'
        ]
        return {i: f for i, f in enumerate(features)}

    def get_order(self):
        return self.feature_order

# ==============================================================================
# æ ¸å¿ƒé€»è¾‘
# ==============================================================================

def fetch_go_source():
    print("\n[æ­¥éª¤1] Go æºç è§£æ")
    
    local_go_path = PROJECT_ROOT / "transform.go"
    if local_go_path.exists():
        print(f"å‘ç°æœ¬åœ° transform.go æ–‡ä»¶: {local_go_path}")
        return local_go_path.read_text(encoding='utf-8')
    
    if GO_SOURCE_CACHE_PATH.exists():
        if (time.time() - GO_SOURCE_CACHE_PATH.stat().st_mtime) < 86400:
            print(f"æˆåŠŸåŠ è½½æœ¬åœ°ç¼“å­˜: {GO_SOURCE_CACHE_PATH}")
            return GO_SOURCE_CACHE_PATH.read_text(encoding='utf-8')

    print(f"æ­£åœ¨ä¸‹è½½ Go æºæ–‡ä»¶: {GO_SOURCE_URL}")
    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        response = requests.get(GO_SOURCE_URL, timeout=10)
        response.raise_for_status()
        content = response.text
        GO_SOURCE_CACHE_PATH.write_text(content, encoding='utf-8')
        print("ä¸‹è½½å¹¶ç¼“å­˜æˆåŠŸ")
        return content
    except Exception as e:
        if GO_SOURCE_CACHE_PATH.exists():
            print(f"ä¸‹è½½å¤±è´¥ ({e})ï¼Œä½¿ç”¨æ—§ç¼“å­˜")
            return GO_SOURCE_CACHE_PATH.read_text(encoding='utf-8')
        raise RuntimeError(f"æ— æ³•è·å– Go æºç : {e}")

def load_data(data_dir, days=90):
    print("\n[æ­¥éª¤2] æ•°æ®åŠ è½½ä¸æ¸…æ´—")
    print(f"å¼€å§‹ä»æ•°æ®ç›®å½•åŠ è½½ CSV æ–‡ä»¶: {data_dir}")
    
    if not data_dir.exists():
        raise FileNotFoundError(f"Data directory not found: {data_dir}")

    all_files = glob.glob(str(data_dir / "*.csv"))
    
    # ç­›é€‰æœ€è¿‘ N å¤©çš„æ•°æ®
    cutoff_time = time.time() - (days * 86400)
    recent_files = []
    
    for f in all_files:
        try:
            mtime = os.path.getmtime(f)
            # å°è¯•ä»æ–‡ä»¶åè§£ææ—¥æœŸ (smart_20250101_1200.csv)
            fname = os.path.basename(f)
            date_match = re.search(r'smart_(\d{8}_\d{4})', fname)
            if date_match:
                file_time = time.mktime(time.strptime(date_match.group(1), "%Y%m%d_%H%M"))
                if file_time > cutoff_time:
                    recent_files.append(f)
            elif mtime > cutoff_time:
                recent_files.append(f)
        except:
            pass

    if not recent_files:
        print("è­¦å‘Š: æœªå‘ç°è¿‘æœŸæ•°æ®ï¼Œå°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®...")
        recent_files = all_files
    
    if not recent_files:
        raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½• CSV æ•°æ®æ–‡ä»¶")

    print(f"--- é€‰ä¸­ {len(recent_files)} ä¸ªæ•°æ®æ–‡ä»¶ ---")
    
    dfs = []
    for f in recent_files:
        try:
            # å®½å®¹æ¨¡å¼è¯»å–
            df = pd.read_csv(f, encoding='utf-8', on_bad_lines='skip')
            
            # è®¡ç®—æ–‡ä»¶å¹´é¾„ï¼ˆå¤©ï¼‰ï¼Œç”¨äºåç»­æƒé‡è¡°å‡
            fname = os.path.basename(f)
            date_match = re.search(r'smart_(\d{8}_\d{4})', fname)
            if date_match:
                file_time = time.mktime(time.strptime(date_match.group(1), "%Y%m%d_%H%M"))
                age_days = (time.time() - file_time) / 86400
            else:
                age_days = (time.time() - os.path.getmtime(f)) / 86400
            
            df['__file_age_days'] = max(0, age_days)
            dfs.append(df)
        except Exception as e:
            print(f"è·³è¿‡æ–‡ä»¶ {os.path.basename(f)}: {e}")
            continue
    
    if not dfs:
        raise ValueError("æ— æ³•åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®")
    
    merged_df = pd.concat(dfs, ignore_index=True)
    print(f"æ•°æ®åˆå¹¶å®Œæˆï¼ŒåŸå§‹è®°å½•æ•°: {len(merged_df)}")
    return merged_df

def preprocess_data(df, feature_order):
    print("\n[æ­¥éª¤3] ç‰¹å¾æå–ä¸ç›®æ ‡æ„å»º (æé€Ÿæ¨¡å¼)")

    # 1. ç¡®å®šç›®æ ‡åˆ— (æˆ‘ä»¬åªçœ‹ MaxDownloadRate)
    target_col = 'maxdownloadrate_kb'
    if target_col not in df.columns:
        # å…¼å®¹æ—§ç‰ˆæœ¬æ•°æ®åˆ—å
        if 'download_mbps' in df.columns:
            df[target_col] = df['download_mbps'] * 1024 # è½¬æ¢ä¸º kb
        else:
            raise ValueError("ä¸¥é‡é”™è¯¯: æ•°æ®ä¸­ç¼ºå°‘ maxdownloadrate_kb åˆ—ï¼Œæ— æ³•è®­ç»ƒé€Ÿåº¦æ¨¡å‹")

    # å¡«å……ç¼ºå¤±å€¼
    df[target_col] = df[target_col].fillna(0)
    
    # --------------------------------------------------------------------------
    # æ ¸å¿ƒé»‘ç§‘æŠ€ï¼šæ„å»º "æƒ©ç½šæ€§" ç›®æ ‡å˜é‡ (Punished Target)
    # --------------------------------------------------------------------------
    # ç›®æ ‡ï¼šè®©æ¨¡å‹é¢„æµ‹çš„å€¼ä¸ä»…ä»…æ˜¯é€Ÿåº¦ï¼Œè€Œæ˜¯ "ç¨³å®šé€Ÿåº¦"ã€‚
    # æ‰‹æ®µï¼šå¦‚æœèŠ‚ç‚¹æœ‰ä¸¢åŒ…æˆ–é«˜å»¶è¿Ÿï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ—¶äººä¸ºæŠŠå®ƒçš„ Target å€¼æ‰“ä½ã€‚
    # ç»“æœï¼šæ¨¡å‹åœ¨æ¨ç†æ—¶ï¼Œä¼šç»™é‚£äº› "å¿«ä½†ä¸¢åŒ…" çš„èŠ‚ç‚¹æ‰“å‡ºå¾ˆä½çš„é¢„æµ‹åˆ†ï¼Œä»è€Œé¿å¼€å®ƒä»¬ã€‚
    # --------------------------------------------------------------------------
    
    raw_speed = df[target_col]
    
    # æƒ©ç½šå› å­ 1: ä¸¢åŒ…æƒ©ç½š
    # failure > 0 æ—¶ï¼Œæƒ©ç½šæå…¶ä¸¥å‰ã€‚failure=1 -> åˆ†æ•°å˜ä¸º 1/3; failure=2 -> åˆ†æ•°å˜ä¸º 1/5
    failure_penalty = 1.0 / (1.0 + df['failure'].fillna(0) * 2.0)
    
    # æƒ©ç½šå› å­ 2: å»¶è¿Ÿæƒ©ç½š
    # å»¶è¿Ÿè¶Šé«˜ï¼Œåˆ†æ•°è¶Šä½ã€‚æ¯ 1000ms å»¶è¿Ÿï¼Œåˆ†æ•°æ‰“ 8 æŠ˜ã€‚
    # ä¸»è¦æ˜¯ä¸ºäº†å‰”é™¤é‚£äº› 2000ms+ çš„å‡æ­»èŠ‚ç‚¹
    latency_val = df['latency'].fillna(10000)
    latency_penalty = 1.0 / (1.0 + (latency_val / 4000.0)) 
    
    # æœ€ç»ˆè®­ç»ƒç›®æ ‡ï¼š(ç‰©ç†é€Ÿåº¦) * (ä¸¢åŒ…æƒ©ç½š) * (å»¶è¿Ÿæƒ©ç½š)
    # è¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ï¼Œé¢„æµ‹å€¼è¶Šé«˜ï¼Œä»£è¡¨èŠ‚ç‚¹ "æ—¢å¿«åˆç¨³"
    y = raw_speed * failure_penalty * latency_penalty
    
    # è®°å½•æ—¥å¿—çœ‹çœ‹æ•ˆæœ
    print(f"ç›®æ ‡æ„å»ºç¤ºä¾‹ (å‰5æ¡):")
    for i in range(min(5, len(df))):
        print(f"  åŸå§‹é€Ÿåº¦: {raw_speed.iloc[i]:.0f} kbps, å¤±è´¥æ•°: {df['failure'].iloc[i]}, "
              f"å»¶è¿Ÿ: {latency_val.iloc[i]:.0f} ms -> è®­ç»ƒç›®æ ‡å€¼: {y.iloc[i]:.2f}")

    # --------------------------------------------------------------------------
    # ç­–ç•¥ä¼˜åŒ–ï¼šæ–°èŠ‚ç‚¹æ¢ç´¢æœºåˆ¶ (Exploration Strategy)
    # é—®é¢˜ï¼šå¦‚æœå®Œå…¨ä¾èµ–å†å²æ•°æ®ï¼Œæ–°èŠ‚ç‚¹(å†å²ä¸º0)ä¼šè¢«æ°¸è¿œæ‰“å…¥å†·å®«ã€‚
    # è§£å†³ï¼šéšæœºå°† 25% æ•°æ®çš„"å†å²ç‰¹å¾"å¼ºè¡Œç½®ä¸º 0ã€‚
    # æ•ˆæœï¼šæ•™ä¼šæ¨¡å‹ "å³ä½¿æ²¡æœ‰å†å²æ•°æ®ï¼Œåªè¦å»¶è¿Ÿä½ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ä¸ªå¥½èŠ‚ç‚¹"ã€‚
    # --------------------------------------------------------------------------
    history_cols = ['history_maxdownloadrate_kb', 'history_download_mb', 'last_used_seconds']
    # åˆ›å»ºä¸€ä¸ªéšæœºæ©ç ï¼Œ25% çš„æ¦‚ç‡ä¸º True
    exploration_mask = np.random.rand(len(df)) < 0.25
    
    for col in history_cols:
        if col in df.columns:
            # å¯¹äºé€‰ä¸­çš„è¡Œï¼Œå°†å†å²ç‰¹å¾æŠ¹å» (æ¨¡æ‹Ÿæˆæ–°èŠ‚ç‚¹)
            df.loc[exploration_mask, col] = 0.0

    # 2. ç‰¹å¾å±è”½ (Masking)
    # å°†ä¸éœ€è¦çš„ç‰¹å¾ç½®ä¸º 0ï¼Œé˜²æ­¢å™ªå£°å¹²æ‰°
    for col in IGNORED_FEATURES:
        if col in df.columns:
            df[col] = 0.0

    # 3. æŒ‰é¡ºåºæå–ç‰¹å¾çŸ©é˜µ X
    ordered_cols = [feature_order[i] for i in sorted(feature_order.keys())]
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
    for col in ordered_cols:
        if col not in df.columns:
            df[col] = 0.0
            
    X = df[ordered_cols].copy()
    
    # åªä¿ç•™æ•°å€¼ç±»å‹
    X = X.select_dtypes(include=np.number)
    
    # 4. ç‰¹å¾æ ‡å‡†åŒ– (Standardization)
    print("\n[æ­¥éª¤4] ç‰¹å¾æ ‡å‡†åŒ–")
    scalers = {}
    
    # æ•°å€¼å‹ç‰¹å¾ -> StandardScaler
    std_cols = [c for c in CONTINUOUS_FEATURES if c in X.columns]
    if std_cols:
        scaler_std = StandardScaler()
        X[std_cols] = scaler_std.fit_transform(X[std_cols])
        scalers['standard'] = scaler_std
        scalers['std_features'] = std_cols
        print(f"StandardScaler åº”ç”¨äº {len(std_cols)} ä¸ªç‰¹å¾")

    # è®¡æ•°å‹ç‰¹å¾ -> RobustScaler
    rob_cols = [c for c in COUNT_FEATURES if c in X.columns]
    if rob_cols:
        scaler_rob = RobustScaler()
        X[rob_cols] = scaler_rob.fit_transform(X[rob_cols])
        scalers['robust'] = scaler_rob
        scalers['rob_features'] = rob_cols
        print(f"RobustScaler åº”ç”¨äº {len(rob_cols)} ä¸ªç‰¹å¾")

    # 5. æ ·æœ¬æƒé‡ (Sample Weights) - ä¼˜åŒ–ï¼šæ—¶é—´ä¸»å¯¼çš„ä¹˜æ³•æƒé‡
    # --------------------------------------------------------------------------
    # ç­–ç•¥ï¼šæŒ‡æ•°çº§æ—¶é—´è¡°å‡
    # Day 0: 100%, Day 1: 82%, Day 3: 55%, Day 7: 25%, Day 14: 6%
    # è¶Šæ—§çš„æ•°æ®ï¼Œå¯¹æ¨¡å‹çš„å½±å“åŠ›å‘ˆæ–­å´–å¼ä¸‹è·Œã€‚
    # --------------------------------------------------------------------------
    time_decay = np.exp(-0.2 * df['__file_age_days'])
    
    # é€Ÿåº¦åŠ æˆï¼šä¾ç„¶ä¿ç•™å¯¹é«˜é€Ÿæ ·æœ¬çš„å…³æ³¨ï¼Œä½†å¿…é¡»å—åˆ¶äºæ—¶é—´è¡°å‡
    # é€Ÿåº¦è¶Šå¿«ï¼Œæƒé‡ä¼šæœ‰ 1.0 ~ 2.0 å€çš„åŠ æˆ
    speed_bonus = np.log1p(raw_speed) / 12.0  
    
    # æœ€ç»ˆæƒé‡ = æ—¶é—´è¡°å‡ç³»æ•° * (åŸºç¡€åˆ† + é€Ÿåº¦åŠ æˆ)
    # ä½¿ç”¨ä¹˜æ³•ï¼šç¡®ä¿æ—§æ•°æ®å³ä½¿é€Ÿåº¦å†å¿«ï¼Œæ€»æƒé‡ä¹Ÿè¢«æ—¶é—´ç³»æ•°å¼ºè¡Œå‹ä½
    sample_weights = time_decay * (1.0 + speed_bonus)

    return X, y, sample_weights, scalers

def save_model_and_params(model, scalers, feature_order, output_path):
    print("\n[æ­¥éª¤7] æ¨¡å‹ä¿å­˜ä¸å‚æ•°æ³¨å…¥")
    
    # ä¿å­˜åŸå§‹ LightGBM æ¨¡å‹
    model.booster_.save_model(str(output_path))
    
    # æ„å»º INI æ ¼å¼çš„å˜æ¢å‚æ•°
    feature_name_to_idx = {v: k for k, v in feature_order.items()}
    
    ini_content = ["", "", "[transforms]"]
    
    # 1. Order åŒºå—
    ini_content.append("[order]")
    for i in sorted(feature_order.keys()):
        ini_content.append(f"{i}={feature_order[i]}")
    ini_content.append("[/order]")
    
    # 2. Definitions åŒºå— (æ ‡å‡†åŒ–å‚æ•°)
    ini_content.append("[definitions]")
    
    # StandardScaler å‚æ•°å†™å…¥
    s_std = scalers.get('standard')
    f_std = scalers.get('std_features', [])
    if s_std and f_std:
        indices = []
        valid_idx = []
        for i, name in enumerate(f_std):
            if name in feature_name_to_idx:
                indices.append(str(feature_name_to_idx[name]))
                valid_idx.append(i)
        
        if indices:
            ini_content.append("std_type=StandardScaler")
            ini_content.append("std_features=" + ",".join(indices))
            
            means = [f"{x:.6f}" for x in s_std.mean_[valid_idx]]
            ini_content.append("std_mean=" + ",".join(means))
            
            scales = [f"{x:.6f}" for x in s_std.scale_[valid_idx]]
            ini_content.append("std_scale=" + ",".join(scales))

    # RobustScaler å‚æ•°å†™å…¥
    s_rob = scalers.get('robust')
    f_rob = scalers.get('rob_features', [])
    if s_rob and f_rob:
        indices = []
        valid_idx = []
        for i, name in enumerate(f_rob):
            if name in feature_name_to_idx:
                indices.append(str(feature_name_to_idx[name]))
                valid_idx.append(i)
        
        if indices:
            ini_content.append("") # ç©ºè¡Œåˆ†éš”
            ini_content.append("robust_type=RobustScaler")
            ini_content.append("robust_features=" + ",".join(indices))
            
            centers = [f"{x:.6f}" for x in s_rob.center_[valid_idx]]
            ini_content.append("robust_center=" + ",".join(centers))
            
            scales = [f"{x:.6f}" for x in s_rob.scale_[valid_idx]]
            ini_content.append("robust_scale=" + ",".join(scales))

    ini_content.append("[/definitions]")
    
    # 3. å¯ç”¨å˜æ¢
    ini_content.append("")
    ini_content.append("transform=true")
    ini_content.append("[/transforms]")
    
    # è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
    with open(output_path, "ab") as f:
        f.write("\n".join(ini_content).encode('utf-8'))
    
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {output_path} (åŒ…å«å®Œæ•´é¢„å¤„ç†å‚æ•°)")

def training_logger(period=100):
    def _callback(env):
        if period > 0 and (env.iteration + 1) % period == 0:
            msg = f"[è¿­ä»£ {env.iteration + 1:5d}]"
            for data_name, eval_name, result, *rest in env.evaluation_result_list:
                msg += f" {data_name}-{eval_name}: {result:.4f}"
            print(msg)
    _callback.order = 10
    return _callback

def main():
    print_separator("Mihomo æé€Ÿæƒé‡æ¨¡å‹è®­ç»ƒå™¨ (Speed & Stability First)")
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=Path, default=DEFAULT_DATA_DIR)
    parser.add_argument("--output", type=Path, default=DEFAULT_MODEL_PATH)
    args = parser.parse_args()

    # 1. è§£æç‰¹å¾
    try:
        go_content = fetch_go_source()
        parser_obj = GoTransformParser(go_content)
        feature_order = parser_obj.get_order()
    except Exception as e:
        print(f"é”™è¯¯: Go æºç è§£æå¤±è´¥: {e}")
        sys.exit(1)

    # 2. åŠ è½½æ•°æ®
    try:
        # é»˜è®¤åªåŠ è½½æœ€è¿‘ 14 å¤©çš„æ•°æ®ï¼Œä¿è¯æ—¶æ•ˆæ€§
        df = load_data(args.data_dir, days=14)
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

    # 3. é¢„å¤„ç† (åº”ç”¨æé€Ÿç­–ç•¥)
    try:
        X, y, weights, scalers = preprocess_data(df, feature_order)
    except Exception as e:
        print(f"é¢„å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

    # 4. åˆ’åˆ†æ•°æ®é›†
    print("\n[æ­¥éª¤5] åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†")
    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(
        X, y, weights, test_size=0.15, random_state=42
    )
    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ¡, éªŒè¯é›†: {X_val.shape[0]} æ¡")

    # 5. è®­ç»ƒ
    print("\n[æ­¥éª¤6] æ¨¡å‹è®­ç»ƒ (LightGBM)")
    model = lgb.LGBMRegressor(**LGBM_PARAMS)
    
    callbacks = [
        lgb.early_stopping(stopping_rounds=100, verbose=True),
        training_logger(period=200)
    ]

    model.fit(
        X_train, y_train,
        sample_weight=w_train,
        eval_set=[(X_val, y_val)],
        eval_sample_weight=[w_val],
        callbacks=callbacks
    )

    if model.best_iteration_ < LGBM_PARAMS['n_estimators']:
         print(f"è®­ç»ƒçŠ¶æ€: è§¦å‘æ—©åœã€‚æœ€ä½³è¿­ä»£è½®æ•°: [{model.best_iteration_}]")
    else:
         print(f"è®­ç»ƒçŠ¶æ€: æœªè§¦å‘æ—©åœ (è·‘æ»¡å…¨é‡)ã€‚æœ€ä½³è¿­ä»£è½®æ•°: [{model.best_iteration_}]")

    # 6. è¯„ä¼°
    preds = model.predict(X_val)
    r2 = r2_score(y_val, preds)
    
    # ç®€å•çš„çº¿æ€§æ˜ å°„è¯„åˆ† (0.0 - 10.0)
    # R2=0.5 -> 5.0åˆ†, R2=0.8 -> 8.0åˆ†
    final_score = max(0, r2 * 10)

    print(f"\nè®­ç»ƒç»“æŸ. æœ€ä½³è¿­ä»£: {model.best_iteration_}")
    print(f"éªŒè¯é›† R2 å¾—åˆ†: {r2:.4f}")
    print(f"æ¨¡å‹æœ€ç»ˆè¯„åˆ†: {final_score:.3f} / 10.0")
    
    if final_score > 8.0:
        print("âœ¨ è¯„çº§: Sçº§ (æä½³) - æé€ŸèŠ‚ç‚¹è¯†åˆ«ç²¾å‡†")
    elif final_score > 6.0:
        print("ğŸŸ¢ è¯„çº§: Açº§ (è‰¯å¥½) - æ¨¡å‹å¯ç”¨æ€§é«˜")
    elif final_score > 4.0:
        print("ğŸŸ¡ è¯„çº§: Bçº§ (åŠæ ¼) - æ­£å¸¸æ°´å¹³")
    elif final_score > 2.0:
        print("ğŸŸ  è¯„çº§: Cçº§ (ä¸€èˆ¬) - éœ€ç§¯ç´¯æ›´å¤šæ•°æ®")
    else:
        print("ğŸ”´ è¯„çº§: Dçº§ (ä¸åˆæ ¼) - å™ªå£°è¿‡å¤§æˆ–æ•°æ®ä¸è¶³")

    # 7. ä¿å­˜
    args.output.parent.mkdir(parents=True, exist_ok=True)
    if args.output.exists():
        args.output.unlink() # åˆ é™¤æ—§æ–‡ä»¶
        
    save_model_and_params(model, scalers, feature_order, args.output)
    
    print_separator("å®Œæˆ")

if __name__ == "__main__":
    main()
